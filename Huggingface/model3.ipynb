{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pickle\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import  DistilBertTokenizer\n",
    "from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"D:\\Datasets\\movie-genre-prediction\\\\train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting the unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train.drop(columns=['Unnamed: 0','movie_name','id'],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['genre_enc'] = encoder.fit_transform(dataset['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genre</th>\n",
       "      <th>genre_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A young scriptwriter starts bringing valuable ...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A director and her friends renting a haunted h...</td>\n",
       "      <td>horror</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an educational video for families and ...</td>\n",
       "      <td>family</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scientists working in the Austrian Alps discov...</td>\n",
       "      <td>scifi</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buy Day - Four Men Widely - Apart in Life - By...</td>\n",
       "      <td>action</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            synopsis    genre  genre_enc\n",
       "0  A young scriptwriter starts bringing valuable ...  fantasy          4\n",
       "1  A director and her friends renting a haunted h...   horror          5\n",
       "2  This is an educational video for families and ...   family          3\n",
       "3  Scientists working in the Austrian Alps discov...    scifi          8\n",
       "4  Buy Day - Four Men Widely - Apart in Life - By...   action          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(dataset['synopsis'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "sequenced_text = tokenizer.texts_to_sequences(dataset['synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(max([len(txt) for txt in sequenced_text]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequence = pad_sequences(sequenced_text, maxlen=70)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_valid, y_train, y_valid = train_test_split(pad_sequence,dataset['genre_enc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation for Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting data into tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "tf_valid = tf.data.Dataset.from_tensor_slices((x_valid,y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"tf_bert_for_sequence_classification\" (type TFBertForSequenceClassification).\n\nin user code:\n\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1569, in call  *\n        outputs = self.bert(\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_file7xm78srg.py\", line 127, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(input_ids=ag__.ld(input_ids), position_ids=ag__.ld(position_ids), token_type_ids=ag__.ld(token_type_ids), inputs_embeds=ag__.ld(inputs_embeds), past_key_values_length=ag__.ld(past_key_values_length), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py\", line 46, in tf__call\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('inputs_embeds',), 1)\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py\", line 41, in if_body_1\n        inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n\n    TypeError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n    \n    in user code:\n    \n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 780, in call  *\n            embedding_output = self.embeddings(\n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py\", line 46, in tf__call\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('inputs_embeds',), 1)\n        File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py\", line 41, in if_body_1\n            inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n    \n        TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).\n        \n        in user code:\n        \n            File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 203, in call  *\n                inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n        \n            TypeError: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64\n        \n        \n        Call arguments received by layer 'embeddings' (type TFBertEmbeddings):\n          • input_ids=tf.Tensor(shape=(None, 70), dtype=float32)\n          • position_ids=None\n          • token_type_ids=tf.Tensor(shape=(None, 70), dtype=int32)\n          • inputs_embeds=None\n          • past_key_values_length=0\n          • training=False\n    \n    \n    Call arguments received by layer 'bert' (type TFBertMainLayer):\n      • input_ids=tf.Tensor(shape=(None, 70), dtype=float32)\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • encoder_hidden_states=None\n      • encoder_attention_mask=None\n      • past_key_values=None\n      • use_cache=None\n      • output_attentions=False\n      • output_hidden_states=False\n      • return_dict=True\n      • training=False\n\n\nCall arguments received by layer \"tf_bert_for_sequence_classification\" (type TFBertForSequenceClassification):\n  • input_ids=tf.Tensor(shape=(None, 70), dtype=float32)\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_layer \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39m70\u001b[39m,))\n\u001b[1;32m----> 2\u001b[0m bert_output \u001b[39m=\u001b[39m bert_model(input_layer)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m output_layer \u001b[39m=\u001b[39m Dense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m)(additional_layers)\n\u001b[0;32m      4\u001b[0m model \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39minput_layer, outputs\u001b[39m=\u001b[39moutput_layer)\n",
      "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9letwcub.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m     15\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mbert, (), \u001b[39mdict\u001b[39;49m(input_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(input_ids), attention_mask\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(attention_mask), token_type_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(token_type_ids), position_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(position_ids), head_mask\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(head_mask), inputs_embeds\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(inputs_embeds), output_attentions\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(output_hidden_states), return_dict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(return_dict), training\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(training)), fscope)\n\u001b[0;32m     18\u001b[0m pooled_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(outputs)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m pooled_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdropout, (), \u001b[39mdict\u001b[39m(inputs\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(pooled_output), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7xm78srg.py:127\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(token_type_ids) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, if_body_6, else_body_6, get_state_6, set_state_6, (\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m embedding_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49membeddings, (), \u001b[39mdict\u001b[39;49m(input_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(input_ids), position_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(position_ids), token_type_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(token_type_ids), inputs_embeds\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(inputs_embeds), past_key_values_length\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(past_key_values_length), training\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(training)), fscope)\n\u001b[0;32m    128\u001b[0m attention_mask_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(shape_list), (ag__\u001b[39m.\u001b[39mld(attention_mask),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m    129\u001b[0m mask_seq_length \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(seq_length) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(past_key_values_length)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py:46\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[39mnonlocal\u001b[39;00m inputs_embeds\n\u001b[0;32m     45\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(input_ids) \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39;49m\u001b[39minputs_embeds\u001b[39;49m\u001b[39m'\u001b[39;49m,), \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     47\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(shape_list), (ag__\u001b[39m.\u001b[39mld(inputs_embeds),), \u001b[39mNone\u001b[39;00m, fscope)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_2\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py:41\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mnonlocal\u001b[39;00m inputs_embeds\n\u001b[0;32m     40\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(check_embeddings_within_bounds), (ag__\u001b[39m.\u001b[39mld(input_ids), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mvocab_size), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 41\u001b[0m inputs_embeds \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mgather, (), \u001b[39mdict\u001b[39;49m(params\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mweight, indices\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(input_ids)), fscope)\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"tf_bert_for_sequence_classification\" (type TFBertForSequenceClassification).\n\nin user code:\n\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1569, in call  *\n        outputs = self.bert(\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_file7xm78srg.py\", line 127, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(input_ids=ag__.ld(input_ids), position_ids=ag__.ld(position_ids), token_type_ids=ag__.ld(token_type_ids), inputs_embeds=ag__.ld(inputs_embeds), past_key_values_length=ag__.ld(past_key_values_length), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py\", line 46, in tf__call\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('inputs_embeds',), 1)\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py\", line 41, in if_body_1\n        inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n\n    TypeError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n    \n    in user code:\n    \n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 780, in call  *\n            embedding_output = self.embeddings(\n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py\", line 46, in tf__call\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('inputs_embeds',), 1)\n        File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filewe838dyo.py\", line 41, in if_body_1\n            inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n    \n        TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).\n        \n        in user code:\n        \n            File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 203, in call  *\n                inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n        \n            TypeError: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64\n        \n        \n        Call arguments received by layer 'embeddings' (type TFBertEmbeddings):\n          • input_ids=tf.Tensor(shape=(None, 70), dtype=float32)\n          • position_ids=None\n          • token_type_ids=tf.Tensor(shape=(None, 70), dtype=int32)\n          • inputs_embeds=None\n          • past_key_values_length=0\n          • training=False\n    \n    \n    Call arguments received by layer 'bert' (type TFBertMainLayer):\n      • input_ids=tf.Tensor(shape=(None, 70), dtype=float32)\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • encoder_hidden_states=None\n      • encoder_attention_mask=None\n      • past_key_values=None\n      • use_cache=None\n      • output_attentions=False\n      • output_hidden_states=False\n      • return_dict=True\n      • training=False\n\n\nCall arguments received by layer \"tf_bert_for_sequence_classification\" (type TFBertForSequenceClassification):\n  • input_ids=tf.Tensor(shape=(None, 70), dtype=float32)\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=None\n  • training=False"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1658, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_file9letwcub.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_file7xm78srg.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1569, in call  *\n            outputs = self.bert(\n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_file7xm78srg.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 766, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'bert' (type TFBertMainLayer):\n          • input_ids=tf.Tensor(shape=(70,), dtype=int32)\n          • attention_mask=None\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification):\n      • input_ids=tf.Tensor(shape=(70,), dtype=int32)\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bert_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mspare_categoricalcrossentropy\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m bert_model\u001b[39m.\u001b[39;49mfit(tf_train)\n",
      "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file19wzg4au.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1658\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1656\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1657\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1658\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_using_dummy_loss:\n\u001b[0;32m   1660\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_loss(y_pred\u001b[39m.\u001b[39mloss, y_pred\u001b[39m.\u001b[39mloss, sample_weight, regularization_losses\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9letwcub.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m     15\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mbert, (), \u001b[39mdict\u001b[39;49m(input_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(input_ids), attention_mask\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(attention_mask), token_type_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(token_type_ids), position_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(position_ids), head_mask\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(head_mask), inputs_embeds\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(inputs_embeds), output_attentions\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(output_hidden_states), return_dict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(return_dict), training\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(training)), fscope)\n\u001b[0;32m     18\u001b[0m pooled_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(outputs)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m pooled_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdropout, (), \u001b[39mdict\u001b[39m(inputs\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(pooled_output), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7xm78srg.py:76\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m     74\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(input_ids) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(inputs_embeds) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_shape)\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n\u001b[0;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1658, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_file9letwcub.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_file7xm78srg.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1569, in call  *\n            outputs = self.bert(\n        File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_filemznqmxu4.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"C:\\Users\\adity\\AppData\\Local\\Temp\\__autograph_generated_file7xm78srg.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 766, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'bert' (type TFBertMainLayer):\n          • input_ids=tf.Tensor(shape=(70,), dtype=int32)\n          • attention_mask=None\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification):\n      • input_ids=tf.Tensor(shape=(70,), dtype=int32)\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "bert_model.compile(optimizer='adam',loss='spare_categoricalcrossentropy',metrics=['accuracy'])\n",
    "bert_model.fit(tf_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
