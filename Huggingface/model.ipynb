{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "import pickle\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import  TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"D:\\Datasets\\movie-genre-prediction\\\\train.csv\")\n",
    "test = pd.read_csv(\"D:\\Datasets\\movie-genre-prediction\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fantasy', 'horror', 'family', 'scifi', 'action', 'crime',\n",
       "       'adventure', 'mystery', 'romance', 'thriller'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.iloc[:,3].values\n",
    "y = train.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000,)\n",
      "(54000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000,)\n",
      "[6 1 7 9 3 2 4 5 0 8]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "y_encoded = np.array(y_encoded)\n",
    "print(y_encoded.shape)\n",
    "labels = list(set(y))\n",
    "y_labels = encoder.fit_transform(labels)\n",
    "print(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = []\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# for i in range(len(x)):\n",
    "#     sentence = re.sub('[^a-zA-Z]',' ',x[i])\n",
    "#     sentence = sentence.lower()\n",
    "#     sentence = sentence.split(' ')\n",
    "#     sentence = [stemmer.stem(word) for word in sentence if word not in set(stopwords.words('english'))]\n",
    "#     sentence = \" \".join(sentence)\n",
    "#     sentences.append(sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the Preprocessed Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pickeled_sentences.pickle','wb') as f:\n",
    "#     pickle.dump(sentences,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickeled_sentences.pickle','rb') as f:\n",
    "    sentences = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the Vocublary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32955\n"
     ]
    }
   ],
   "source": [
    "tokens = set()\n",
    "for i in range(len(sentences)):\n",
    "    token = word_tokenize(sentences[i])\n",
    "    for j in token:\n",
    "        tokens.add(j)\n",
    "print(len(tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding of the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127, 31439, 28306, 25830, 29978, 7783, 10487, 18525, 31052, 3779, 18112, 8186, 3968, 3951]\n",
      "[16151, 27827, 28636, 19288, 10630, 12191, 28919, 1242, 1966, 16952, 9226, 20768]\n",
      "[24951, 21342, 31657, 31657, 25243, 19682, 25968, 31657, 8629, 30718, 15658, 16113, 6574, 31868]\n",
      "[2167, 10853, 18774, 8885, 13703, 2131, 908, 10126, 4054, 11462, 24899, 23095]\n",
      "[10290, 13528, 9548, 28851, 11362, 682, 28988, 24120, 18235, 15586, 29357, 15324, 21315, 24209, 24456, 18346, 10342]\n"
     ]
    }
   ],
   "source": [
    "one_hot_repr = [one_hot(word,32956) for word in sentences]\n",
    "\n",
    "for i in range(5):\n",
    "    print(one_hot_repr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(max(one_hot_repr)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "padded_sentences = pad_sequences(one_hot_repr,30, padding='post')\n",
    "\n",
    "X = np.array(padded_sentences)\n",
    "X_final = np.array(X)\n",
    "y_final = np.array(y_encoded)\n",
    "\n",
    "print(type(X_final))\n",
    "print(type(y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X_final,y_final, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a LSTM Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Embedding(32956,13,input_length=30))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dense(10,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1350/1350 - 50s - loss: 2.1362 - accuracy: 0.1895 - val_loss: 2.0095 - val_accuracy: 0.2515 - 50s/epoch - 37ms/step\n",
      "Epoch 2/30\n",
      "1350/1350 - 46s - loss: 1.9247 - accuracy: 0.2831 - val_loss: 2.0104 - val_accuracy: 0.2637 - 46s/epoch - 34ms/step\n",
      "Epoch 3/30\n",
      "1350/1350 - 49s - loss: 1.8081 - accuracy: 0.3400 - val_loss: 1.9533 - val_accuracy: 0.2982 - 49s/epoch - 37ms/step\n",
      "Epoch 4/30\n",
      "1350/1350 - 48s - loss: 1.6711 - accuracy: 0.4016 - val_loss: 1.9451 - val_accuracy: 0.3081 - 48s/epoch - 35ms/step\n",
      "Epoch 5/30\n",
      "1350/1350 - 46s - loss: 1.5673 - accuracy: 0.4432 - val_loss: 1.9901 - val_accuracy: 0.3028 - 46s/epoch - 34ms/step\n",
      "Epoch 6/30\n",
      "1350/1350 - 50s - loss: 1.4846 - accuracy: 0.4780 - val_loss: 2.0096 - val_accuracy: 0.3006 - 50s/epoch - 37ms/step\n",
      "Epoch 7/30\n",
      "1350/1350 - 46s - loss: 1.4139 - accuracy: 0.5047 - val_loss: 2.1484 - val_accuracy: 0.3018 - 46s/epoch - 34ms/step\n",
      "Epoch 8/30\n",
      "1350/1350 - 47s - loss: 1.3496 - accuracy: 0.5322 - val_loss: 2.1891 - val_accuracy: 0.2958 - 47s/epoch - 34ms/step\n",
      "Epoch 9/30\n",
      "1350/1350 - 47s - loss: 1.2892 - accuracy: 0.5553 - val_loss: 2.2780 - val_accuracy: 0.2906 - 47s/epoch - 35ms/step\n",
      "Epoch 10/30\n",
      "1350/1350 - 46s - loss: 1.2332 - accuracy: 0.5745 - val_loss: 2.3114 - val_accuracy: 0.2888 - 46s/epoch - 34ms/step\n",
      "Epoch 11/30\n",
      "1350/1350 - 46s - loss: 1.1822 - accuracy: 0.5930 - val_loss: 2.4638 - val_accuracy: 0.2872 - 46s/epoch - 34ms/step\n",
      "Epoch 12/30\n",
      "1350/1350 - 48s - loss: 1.1354 - accuracy: 0.6074 - val_loss: 2.5074 - val_accuracy: 0.2782 - 48s/epoch - 35ms/step\n",
      "Epoch 13/30\n",
      "1350/1350 - 46s - loss: 1.0884 - accuracy: 0.6213 - val_loss: 2.5932 - val_accuracy: 0.2747 - 46s/epoch - 34ms/step\n",
      "Epoch 14/30\n",
      "1350/1350 - 43s - loss: 1.0491 - accuracy: 0.6340 - val_loss: 2.6396 - val_accuracy: 0.2724 - 43s/epoch - 32ms/step\n",
      "Epoch 15/30\n",
      "1350/1350 - 43s - loss: 1.0102 - accuracy: 0.6458 - val_loss: 2.7933 - val_accuracy: 0.2635 - 43s/epoch - 32ms/step\n",
      "Epoch 16/30\n",
      "1350/1350 - 43s - loss: 0.9706 - accuracy: 0.6565 - val_loss: 2.8119 - val_accuracy: 0.2637 - 43s/epoch - 32ms/step\n",
      "Epoch 17/30\n",
      "1350/1350 - 43s - loss: 0.9383 - accuracy: 0.6647 - val_loss: 3.0315 - val_accuracy: 0.2627 - 43s/epoch - 32ms/step\n",
      "Epoch 18/30\n",
      "1350/1350 - 43s - loss: 0.9047 - accuracy: 0.6734 - val_loss: 3.0098 - val_accuracy: 0.2551 - 43s/epoch - 32ms/step\n",
      "Epoch 19/30\n",
      "1350/1350 - 47s - loss: 0.8736 - accuracy: 0.6811 - val_loss: 3.2058 - val_accuracy: 0.2590 - 47s/epoch - 35ms/step\n",
      "Epoch 20/30\n",
      "1350/1350 - 43s - loss: 0.8477 - accuracy: 0.6890 - val_loss: 3.2299 - val_accuracy: 0.2544 - 43s/epoch - 32ms/step\n",
      "Epoch 21/30\n",
      "1350/1350 - 43s - loss: 0.8212 - accuracy: 0.6951 - val_loss: 3.3119 - val_accuracy: 0.2527 - 43s/epoch - 32ms/step\n",
      "Epoch 22/30\n",
      "1350/1350 - 43s - loss: 0.7985 - accuracy: 0.7028 - val_loss: 3.4109 - val_accuracy: 0.2470 - 43s/epoch - 32ms/step\n",
      "Epoch 23/30\n",
      "1350/1350 - 43s - loss: 0.7751 - accuracy: 0.7087 - val_loss: 3.5732 - val_accuracy: 0.2494 - 43s/epoch - 32ms/step\n",
      "Epoch 24/30\n",
      "1350/1350 - 44s - loss: 0.7571 - accuracy: 0.7146 - val_loss: 3.6313 - val_accuracy: 0.2490 - 44s/epoch - 32ms/step\n",
      "Epoch 25/30\n",
      "1350/1350 - 43s - loss: 0.7336 - accuracy: 0.7222 - val_loss: 3.7837 - val_accuracy: 0.2439 - 43s/epoch - 32ms/step\n",
      "Epoch 26/30\n",
      "1350/1350 - 44s - loss: 0.7173 - accuracy: 0.7253 - val_loss: 3.9205 - val_accuracy: 0.2430 - 44s/epoch - 33ms/step\n",
      "Epoch 27/30\n",
      "1350/1350 - 43s - loss: 0.6973 - accuracy: 0.7303 - val_loss: 3.7600 - val_accuracy: 0.2424 - 43s/epoch - 32ms/step\n",
      "Epoch 28/30\n",
      "1350/1350 - 43s - loss: 0.6833 - accuracy: 0.7340 - val_loss: 4.0586 - val_accuracy: 0.2439 - 43s/epoch - 32ms/step\n",
      "Epoch 29/30\n",
      "1350/1350 - 43s - loss: 0.6668 - accuracy: 0.7393 - val_loss: 4.0285 - val_accuracy: 0.2421 - 43s/epoch - 32ms/step\n",
      "Epoch 30/30\n",
      "1350/1350 - 44s - loss: 0.6507 - accuracy: 0.7434 - val_loss: 4.1285 - val_accuracy: 0.2368 - 44s/epoch - 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc19303710>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.compile(optimizer= 'adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(x_train,y_train,validation_data=(x_val,y_val),epochs=30,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tensor = tf.constant(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = train['synopsis'].to_list()\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "\n",
    "for text, label in zip(text_data,y_labels):\n",
    "    encoded_input = tokenizer.encode_plus(text,add_special_tokens=True,max_length=128,padding='max_length',truncation=True,return_tensors='tf')\n",
    "\n",
    "input_ids = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "preprocessed_data.append({\n",
    "    'input_ids':input_ids,\n",
    "    'attention_masks': attention_mask,\n",
    "    'label': label\n",
    "})\n",
    "\n",
    "preprocessed_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: preprocessed_data,\n",
    "    output_signature={\n",
    "        'input_ids': tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "        'attention_mask': tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "        'label': tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Shuffle, batch, and prefetch the dataset\n",
    "batch_size = 32\n",
    "preprocessed_dataset = preprocessed_dataset.shuffle(len(preprocessed_data)).batch(batch_size)\n",
    "preprocessed_dataset = preprocessed_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Convert the preprocessed data to TensorFlow Dataset\n",
    "\n",
    "# preprocessed_data = tf.data.Dataset.from_tensor_slices(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "# input_ids = tokenized_inputs[\"input_ids\"]\n",
    "# attention_mask = tokenized_inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels))\n",
    "# dataset = dataset.shuffle(len(sentences)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=CategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m steps_per_epoch \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(preprocessed_data) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m batch_size\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(preprocessed_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch)\n",
      "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1697\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1695\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1696\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1697\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1698\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnexpected result of `train_function` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1699\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(Empty logs). Please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1700\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1701\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1702\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minformation of where went wrong, or file a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39missue/bug to `tf.keras`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1704\u001b[0m     )\n\u001b[0;32m   1705\u001b[0m \u001b[39m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[0;32m   1706\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_and_get_metrics_result(logs)\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(preprocessed_data) // batch_size\n",
    "model.fit(preprocessed_dataset, epochs=10, steps_per_epoch=steps_per_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
